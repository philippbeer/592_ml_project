#+TITLE: COMP-592DL (Section 1) Project in Data Science
#+AUTHOR: Philipp Beer
#+EMAIL: beer.p@live.unic.ac.cy
#+OPTIONS: toc:nil
#+OPTIONS: toc:nil
#+OPTIONS: num:2

#+LATEX_HEADER: \usepackage{amsmath,amssymb,amsfonts}
#+LATEX_HEADER: \usepackage{url}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \DeclarePairedDelimiter\abs{\lvert}{\rvert}
#+LATEX_HEADER: \usepackage[margin=2.5cm]{geometry}
#+LATEX_HEADER: \usepackage{tabularx}
#+LATEX_HEADER: \usepackage{multicol}
#+LATEX_HEADER: \usepackage{parskip}p
#+LATEX_CLASS_OPTIONS: [a4paper,hidelinks,10pt]
#+LATEX_HEADER: \usepackage[AUTO]{inputenc}
#+STARTUP: latexpreview
#+PROPERTY: header-args :exports none :tangle "~/Dropbox/bibliography/592_project.bib"
#+LATEX_HEADER: \usepackage[natbib=true,citestyle=ieee]{biblatex} \DeclareFieldFormat{apacase}{#1} \addbibresource{~/Dropbox/bibliography/592_project.bib}

#+LATEX: \begin{multicols}{2}
* Research Project
** Introduction
<<sec:intro>>
This project paper provides information on the impact of time series clustering on a machine learning (ML) forecasting method. Accurate point forecasts (PFs) for time series data are a challenging task for machine learning methods which still get regularly outperformed by much simpler statistical approaches. As shown in cite:Makridakis_2020, all machine learning methods submitted for the M4 competition had lower accuracy than the used combined benchmark method which consists of statistical methods only. Even worse the complexity of machine learning methods adds significant computational costs to a model without providing a useful reward for it.\\

One question in this context is, whether time series can be combined in such a way that training one model with the appropriate subset simplifies the learning process and increases the point forecast accuracy. Machine Learning models usually have a need for lots of data to be able to perform well in their prediction tasks. For time series a long history is usually a rare commodity and seldomly available. Other approaches are needed if one wants to use ML methods. Therefore, it is interesting to verify whether time series that have similar properties provide additional PF accuracy when trained with the same ML model. A challenge is to choose the right time series properties to simplify the learning task for the algorithm. If done well it should allow for less complex models to achieve better performance by having more data to optimize the model parameters.
** Approach
<<sec:approach>>
A large pool of time series is available in the Makridakis competition. For this research project the daily series of the M4 competition was chosen to analyze whether a simple kMeans algorithm can help to cluster time series via its properties such that the same algorithm can perform more accurate forecasts with a high degree of certainty.
*** Properties
To identify the most relevant time series properties that may be useful in forecasting the tsfresh package developed and described by cite:christ16_distr_paral_time_series_featur was used to compute around 800 different metrics ranging from min, max, number of peaks, autocorrelations, and many others. These properties describe the corresponding time series in their behavior. The assumption in this project is that these properties reveal information that can be used to combine time series for forecasting with machine learning algorithms. Similar properties should make the learning process for machine learning models easier and allow to provide more data compared to training models with individual time series. It assumed that it is also more effective than training the model on all available time series.\\

To find the most relevant properties for a time series the engineered features are filtered via finding the difference between class conditional distributions for different target values. The full mechanism is described in cite:christ16_distr_paral_time_series_featur. A features is deemed relevant if the conditional distributions for the feature $X$ are different for the target values $Y_1$ compared to $Y_2$; in other words if $X$ and $Y$ are not statiscally independent:
#+begin_export latex
\begin{equation}
\begin{aligned}
\exists ~ y_1, y_2 \text{ with } & f_Y(y_1) > 0,\\
& f_Y(y_2) > 0 : f_{X|Y=y_1} \ne f_{X|Y=y_2}
\end{aligned}
\end{equation}
#+end_export
Therefore, a target variable needs to be provided in order to measure the relevance. To avoid being impacted by randomness by selecting only a single target variable for each series the time series is transformed into sliding windows and the subsequent value of the time series is used as the target for each window.\\

After constructing the list of the relevant features for each series the properties where combined into a union of all relevant properties. The features were then extracted once again on the whole time series. This set of engineered features is then used as the dimensions for the clustering algorithm.

 
*** Clustering with K-Means
The idea is to find a prototype for each time series to which it belongs. K-means is chosen as forecasting algorithm to compute centroids of similar time series based on the time series properties. With K-Means it is not clear which value $K$ should have. Therefore all k between 2 and 20 are computed and the silhouette scores $s$ of the resulting class associations for each time series are measured:
#+begin_export latex
\begin{equation}
s(i) = \frac{b(i) - a(i)}{{\max\{a(i),b(i)\}}}
\end{equation}
#+end_export
where $a$ is the mean distance to the other instances in the same cluster, and $b$ is the distance mean nearest cluster distance. In consequence values closer to one are better. To avoid picking a cluster that generates good scores by chance the top 3 $K$ identified via silhouette scores are used for forecasting.\\

*** Cross-Validation
To increase certainty about the belief that any potential improvement generated via the clustered time series is the result of combining alike time series one has to ensure that the better performance is not due to the specific peculiarities of the data. This can be achieved via cross-validating the forecasting models per trained cluster. In this case a 10 fold cross validation is performed on the daily training data of the M4 competition. Specifically, the training set is increased with every fold from the starting point of the training set. This mimics the real world reality that datasets are continuously growing over time and increasingly more data can be used for training. For each fold the chosen 7 steps ahead forecast is generated and the respecitive error metrics discussed in [[sec:benchmarking]] are computed and averaged over the different folds. To compare against the real world performance the full training set is separately trained once again and its error metrics computed with the corresponding M4 test set. This allows to show whether the computed cross-validation metrics hold in a real world scenario given the clustering of the series.
*** Forecasting
To allow for comparison of the clustering performance the following forecasts time series setups are generated: (1) A model with all time series is trained to allow for a base line view of forecasting accuracy. (2) The top 3 kMeans k-clusters identified via the silhouette scores are trained with one model for each cluster to verify whether clustering allows for an improvement over training on the entire dataset. (3) For each k and its corresponding classes a set of the same of amount of classes is generated by drawing random time series from the used data set for each class. The classes also correspond in class size to its counterpart k-clustering, meaning the number of time series inside each particular randomly selected class match the respective number of classes in the kMeans cluster. If the random clusters perform similar to the k-clustered segments one cannot infer that the clustering helped with any possible improvement but rather that less time series per model simplify the learning process.\\

The machine learning model chosen for the forecasts is a relatively simple neural network with 3 hidden layers. The loss function is defined as mean squared error. Per model 100 epochs are executed and the batch size is set to 128.

** Benchmarking
<<sec:benchmarking>>
For the benchmarking of the results the error metrics of the M4 are employed, namely symmetric mean absolute percentage error (sMAPE):
#+begin_export latex
\begin{equation}
SMAPE = \frac{100}{n} \sum_{t=1}^{n} \frac{F_t - Y_t}{(\abs{F_t} + \abs{Y_t})/2}
\end{equation}
#+end_export
where $F_t$ is the forecasted value and $Y_t$ is the actual value at time /t/. The denominator consists of the sum of absolute values of the forecast and the actual value divided by 2. The second metric is the mean absolute scaled error:
#+begin_export latex
\begin{equation}
MASE = mean \left( \frac{\abs{e_j}}{\frac{1}{T-1} \sum_{t=2}^{T} \abs{Y_t - Y_{t-1}}} \right)
\end{equation}
#+end_export
where $e_j$ is the forecast error for a given period of /J/ forecasts, defined as $e_j = Y_j - F_j$, where $Y_j$ are the actual values and $F_j$ are the forecasted values. The denominator consists of the mean absolute error (MAE) of the naive forecast, defined as $Y_t - Y_{t-1}$ computed on the training set /T/ from 1 to /t/. $Y_t$ and $Y_{t-1}$ represent the actual values of the training set.

For the used daily series from the M4 competition a 7-step forecast is generated for 7 consecutive steps increasing the last forecast step to 14 steps ahead. These metrics are employed for both the cross validation computation as well as the comparison to the test test.

** Challenges
The data pipeline consists of the following major parts: (1) preprocessing, (2) feature extraction and selection, (3) clustering, (4) forecasting, (5 ) postprocessing. Each area entails its own set of challenges.

*** Data Preprocessing
Loading and preprocessing the data is required such that the data format matches the expectation of the tsfresh package to be able to compute the time series properties. The M4 dataset is presented in a wide-format layout, whereas tsfresh expects a long format. Preprocessing also included normalizing the data for the utilization via neural network. The /Min-Max feature scaling/ method was chosen and computed on the training set per time series. The scaling is computed via:
#+begin_export latex
\begin{equation}
X' = \frac{X - X_{min}}{X_{max} - X_{min}}
\end{equation}
#+end_export
Not using the test set for the scaling of the values is important as otherwise information leakage from the test set to the training set occurs. In the real world it is unknown whether future values will be larger or smaller than all values observed thus far. An unintended consequence of this computation approach is, that it cannot be guaranteed that the computed values will remain between 0 and 1 - as preferred for the usage within neural networks - between the train and the test set.  This issue is made worse via the utilization of cross validation. With this approach even less records are present in the train set increasing the likelyhood that the test set contains values that exceed the thresholds used for the normalization.
*** Feature Extraction
The tsfresh packages allows for various different metrics to be computed that are separated into various categories (Comprehensive, Minimal, Efficient, Timebased, IndexBased) which can be chosen to be computed. Computational complexity will be discussed in section [[subsec:comp_cost]]. However, due to the constraints on the availability of compute resources the efficient parameter setting was selected as starting point for feature identification for each series. And the relevant subset of those features was computed for all series again.
*** Computational costs
<<subsec:comp_cost>>
The data pipeline described is computationally expensive. In order to be able to train  one of the  M4 datasets in full a cloud computing unit with 6 vCPU and 32GB of RAM was chosen for this task. The tsfresh feature extraction and selection takes on average 40 seconds per time series. With 4227 time series in the M4 daily dataset this initial step takes close to two days. For the clustering a maximum of 20 clusters is selected to keep the required execution time within an acceptable bound. The neural network has a simple setup with 3 hidden layers. Considering cross-validation for the training each time series is used in training for a different neural network 67 times.

Tsfresh as well as the clustering of scikit-learn already implement multiprocessing to improve the time required for the computations. For the various trained neural network a multiprocessing pool is implemented to train the separate models simultaneously as well.

** Results
The results indicate that not training all time series with a single model improves the forecasting performance. Training all series via a single model resulted in sMAPE of 4.55 and MASE 4.68. The best clustered metric was k equal to 4 with sMAPE of 4.02 and MASE 4.45. However, this is likely not attributable to the clustering but due to the reduced number of time series used per model as it does not outperform the randomly created clusters. They perform almost identical in error. Except for $K$ equal to 2 the clustered series slightly outperform the randomly selected clusters. However, the differences are insignificant.

#+CAPTION: M4 daily series error metric for test set
#+NAME: fig:m4_error
[[./img/daily_m4_results.png]]

The error metrics that are generated as averages from the cross validation steps are worse for the K-Means clusters and indicate that choosing clusters of alike series makes accuracy worse compared to selecting them randomly. For both metrics the randomly selected classes peform better than the series combined via clustering.

#+CAPTION: M4 daily series error metrics for cross validation
#+NAME: fig:cv_error
[[./img/daily_cv_results.png]]

** Conclusion
The clustering of the time series as presented requires additional consideration before it could add value to forecasting. And within cross-validation the performance for models using clustered series was worse than random combinations of series. Currently the properties of the clustered series play only an indirect role with the model training. Possibly this is related to the fact that the properties themselves are not considered as inputs for the.\\

Also the K-model selection via silhouette scores possibly is not the best metric for choosing which clusters to use. This could verified via using different clustering algorithms and selection methods.

** Outlook
Looking forward the following aspects deserve attention for future analysis. The complete M4 dataset needs be trained using this approach. And the forecast needs to be aligned to the M4 forecasting mechanism of predicting each forecasting point in the test set once. Furthermore, prediction intervals should be computed and compared using this clustering approach.\\

Other clustering algorithms like agglomerative hierarchical clustering and density based methods need to be considered as well. The cross-validation method could be varied to see whether using chunks changes the performance of the computed errors.\\

It should also be considered whether a meaningful general ranking for time series features can be created that allows to reduce the number of features that need to be computed, based on the forecasting task at hand. Lastly, the implementation of the feature computation can be addressed and moved to a more performant framework outside Python.

#+LATEX: \end{multicols}

** References
:PROPERTIES:
:UNNUMBERED: t
:END:
#+LATEX: \printbibliography[heading=none]


** Time Series Topic Ideas
*** Measure Impact of preprocessing on forecasting algorithm performance
1. identify 4/5/6 forecasting algorithms (from stats and ML)
2. Identify a subset of M5 time series
3. define several data pre-processing steps (de-seasonalization, de-trending, outlier treatment, ...)
4. train algorithms on variation of the pre-processed data
5. validate their performance on the test set
6. compare forecasting results with regards to pre-processing steps
*** Using Time Series Forecasting in Signal Processing for Predictive Maintenance
- safdasdf
*** Integrate Weather Forecast into Retail Sales Product Category forecast
*** 
** Purpose Statement
The purpose of this study is to...
**

** Problem Description
In this project I will investigate the possible cross-validation between the data sets in the M4 competition through an analysis of selected time series properties and seek a method to categorize them into highly correlated groups.
** Makridakis - Research Problem
Categorize M4 time-size to find cross-correlation between them to find time series that are highly correlated

*** Project Intend
In this project I will investigate whether similarities in the time series from the M4 competition can be found, and whether time series data can be categorized into similar groups which allow for a performance improvement in forecasting applications.

** Keywords
- time series cross-validation
- time series correlation
- time series analysis
- auto-correlation function
- time series forecasting
- financial forecasting
- combining forecasts
** Concepts
- Hurst exponent

** Assumptions
- only same time-span considered together - different granularity kept separate
** Questions to Makridakis
- research project clarification

- papers I have found so far?

- 
  
- papers for review of combining approaches (authors or specific papers)
- Hurst exponent - Range sliding (How many to select?)
- how many different ones should I try? Can you help me classify?
- before comparing I would deseasonalize and de-trend -> right approach?
- try 3-4 different clustering techniques to see if something captures them well (k-means, knn)
** Process
*** Assumptions
- no introduction of explanatory values
*** Classify raw time series data
- Outliers yes/no
- trend present?
- seasonality present? (try different sets)
- auto-correlation present?
- compute Hurst exponent
- amount of randomness
*** Pre-processing of M4
- what about outliers? (leave them in or kill them)-> proposal replace them with 0.001/0.999 quantile
- Missing values -> depending on outliers replace them with median or mean
- detrend and deasonalize the data?? or use that for classification?
- scaling to allow better comparison
*** Cluster time-series on extracted features
- use kmeans algorithm to identify segments
*** Utilize statistical methods and ML methods for forecasting
Utilize cross-validation for an MLP to identify generalization error.
- compare when training NN with clustered series and when utilizing individual series
- set baseline with Naive1/ Naive S/ Naive2 and SES and combination of them to have 

*** Comparison
- set the baseline of performance expectation with statistical forecasts
- measure overall performance of each algorithm
- see if any improvement is achieved by clustering based on the criteria and feeding the forecasts

** Mail - Makridakis
Dear Prof. Markidakis.

I have finally managed to put a couple of thoughts around the data science project for my course 592 and would like to present you my current thinking and ask for your help in clarifying the goals and methods to be used a little.

My current draft title is the following:
"In this project I will investigate whether the following properties/methods (list to be inserted here) applied to the M4 time series data set can be used to group those time-series into clusters helping machine learning forecasting methods to improve accuracy and prediction interval accuracy.

Method
 The series clustered into the same group will be forecast using a Multi-Layer-Perceptron and evaluated whether the clustering leads to an improvement in forecasting accuracy and prediction intervals. The data set from the M4 competition can be found, and whether time series data can be categorized into similar groups which allow for a performance improvement in forecasting applications."

I have so far identified the following roperties/methods for analysis:
- Outliers yes/no
- trend present?
- seasonality present? (try different sets)
- auto-correlation present?
- compute Hurst exponent
- amount of randomness

I am considering applying the following pre-processing steps before any analysis on the time-series


